[cassandra]
stop_cmd = {{ cassandra_stop_command }}
start_cmd = {{ cassandra_start_command }}
config_file = {{ cassandra_yml_file }}
cql_username = {{ cassandra_shell_user }}
cql_password = {{ cassandra_shell_password }}
nodetool_username = {{ cassandra_shell_user }}
nodetool_password = {{ cassandra_shell_password }}
;;nodetool_password_file_path = <path to nodetool password file>
;;nodetool_host = <host name or IP to use for nodetool>
;;nodetool_port = <port number to use for nodetool>
;
;; Command ran to verify if Cassandra is running on a node. Defaults to "nodetool version"
;;check_running = nodetool version
;

[storage]
#storage_provider = s3_us_east_2
storage_provider = s3
;; storage_provider should be either of "local", "google_storage" or the s3_* values from
;; https://github.com/apache/libcloud/blob/trunk/libcloud/storage/types.py
;
;; Name of the bucket used for storing backups
bucket_name = {{ medusa_aws_backup_bucket_name }}
;
;; JSON key file for service account with access to GCS bucket or AWS credentials file (home-dir/.aws/credentials)
key_file = {{ medusa_aws_credentials_file }}
;
;; Path of the local storage bucket (used only with 'local' storage provider)
;;base_path = /path/to/backups
;
;; Any prefix used for multitenancy in the same bucket
prefix = {{ medusa_aws_backup_cluster_prefix }}
;
;;fqdn = <enforce the name of the local node. Computed automatically if not provided.>
;
;; Number of days before backups are purged. 0 means backups don't get purged by age (default)
;max_backup_age = 0
;; Number of backups to retain. Older backups will get purged beyond that number. 0 means backups don't get purged by count (default)
;max_backup_count = 0
;; Both thresholds can be defined for backup purge.
;
;; Used to throttle S3 backups/restores:
;transfer_max_bandwidth = 50MB/s
;
;; Max number of downloads/uploads. Not used by the GCS backend.
;concurrent_transfers = 1
;
;; Size over which S3 uploads will be using the awscli with multi part uploads. Defaults to 100MB.
;multi_part_upload_threshold = 104857600
;
;; in the case awscli binaryis not located in the default python path  i.e venv supply the path to binary if the string 'dynamic'
;; is supplied medusa will attempt to the location awscli binary find by looping through directories
;;aws_cli_path=/path/to/awsclibinary/aws
;
;[monitoring]
;;monitoring_provider = <Provider used for sending metrics. Currently either of "ffwd" or "local">
;
;[ssh]
;;username = <SSH username to use for restoring clusters>
;;key_file = <SSH key for use for restoring clusters. Expected in PEM unencrypted format.>
;;port = <SSH port for use for restoring clusters. Default to port 22.
;
;[checks]
;;health_check = <Which ports to check when verifying a node restored properly. Options are 'cql' (default), 'thrift', 'all'.>
;;query = <CQL query to run after a restore to verify it went OK>
;;expected_rows = <Number of rows expected to be returned when the query runs. Not checked if not specified.>
;;expected_result = <Coma separated string representation of values returned by the query. Checks only 1st row returned, and only if specified>
;
;[logging]
;; Controls file logging, disabled by default.
;; enabled = 0
;; file = medusa.log
;; level = INFO
;
;; Control the log output format
;; format = [%(asctime)s] %(levelname)s: %(message)s
;
;; Size over which log file will rotate
;; maxBytes = 20000000
;
;; How many log files to keep
;; backupCount = 50
