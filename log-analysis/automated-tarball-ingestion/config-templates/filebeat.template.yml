###################### Filebeat Configuration Template  #########################
# see cassandra.toolkit/log-analysis/FilebeatSetup.MD for more documentation, comments, and examples
# This template differs on some points from example given in FilebeatSetup.MD however

#=========================== Filebeat inputs =============================

#filebeat.inputs:
  # set in the python script

#==================== Elasticsearch template setting ==========================
setup.template.settings:
  index.number_of_shards: 1
  _source.enabled: false

#============================== Dashboards =====================================
setup.dashboards.enabled: false

#============================== Kibana =====================================
setup.kibana:
  host: "http://localhost:5601"
#================================ Outputs =====================================
# to debug comment out output.logstash and uncomment this. We're using logstash not Elasticsearch for ingestion
# output.console.pretty: true

#----------------------------- Elasticsearch output --------------------------------
output.elasticsearch:
  hosts: ["localhost:9200"]

#----------------------------- Logstash output --------------------------------
# output.logstash:
#   enabled: true
#   hosts: ["localhost:5044"]
#   timeout: 15

#================================ Processors =====================================
# Configure processors to enhance or manipulate events generated by the beat.

processors:
# temporarily removing gc logs and system logs until we can consistently get them in our log tarballs
#------------------------------------- system messages 
  - dissect:
      tokenizer: "%{timestamp} %{+timestamp->} %{+timestamp} %{+timestamp} %{system-host-name} %{component}: %{message}"
      field: "message"
      target_prefix: "ingest"

  # auth.log
  - dissect:
      tokenizer: "%{timestamp} %{+timestamp} %{+timestamp} %{system-host-name} %{component}: %{message}"
      field: "message"
      target_prefix: "ingest"

  - dissect:
      # <path_for_client> will be replaced by the python script
      tokenizer: "<path_for_client>/incident-%{incident-id}/%{host-name}/system/"
      field: "log.file.path"
      target_prefix: "ingest"

#-#------------------------------------ cassandra   
  - dissect:
      tokenizer: "%{loglevel} [%{component}] %{timestamp} %{+timestamp},%{} %{class}.%{}:%{} - %{message}"
      field: "message"
      target_prefix: "ingest"
  - dissect:
      # <path_for_client> will be replaced by the python script
      tokenizer: "<path_for_client>/incident-%{incident-id}/%{host-name}/cassandra/"
      field: "log.file.path"
      target_prefix: "ingest"
      
#-------------------------------------- spark  
  - dissect:
      tokenizer: "%{loglevel} %{timestamp} %{+timestamp},%{} %{class}.%{}:%{} - %{message}"
      field: "message"
      target_prefix: "ingest"
      # TODO consider adding to other dissect statements as well. Though doesn't seem to be working
      trim_values: all

  - timestamp:
      field: "ingest.timestamp"
      layouts:
        - '2006 Jan _2 15:04:05'
        - '2006-01-02 15:04:05'
        # in case there is leading whitespace we didn't strip
        - ' 2006-01-02 15:04:05'
      test:
        - '2020 Jun  8 02:40:01'
        - '2001 Jan 17 15:04:01'
        - '2019-06-22 16:33:51'
        - '2020-06-13 12:57:30'
      ignore_failure: true
      target_field: "@timestamp"
  - include_fields:
      fields: [ "ingest", "message","log", "input", "tags"]
